<p align="center">
  <a href="https://acuvity.ai">
    <picture>
      <img src="https://mma.prnewswire.com/media/2544052/Acuvity__Logo.jpg" height="90" alt="Acuvity logo"/>
    </picture>
  </a>
</p>
<p align="center">
  <a href="https://discord.gg/BkU7fBkrNk">
    <img src="https://img.shields.io/badge/Acuvity-Join-7289DA?logo=discord&logoColor=fff" alt="Join Acuvity community" />
  </a>
<a href="https://www.linkedin.com/company/acuvity/">
    <img src="https://img.shields.io/badge/LinkedIn-Follow-7289DA" alt="Follow us on LinkedIn" />
  </a>
<a href="https://bsky.app/profile/acuvity.bsky.social">
    <img src="https://img.shields.io/badge/Bluesky-Follow-7289DA"?logo=bluesky&logoColor=fff" alt="Follow us on Bluesky" />
  </a>
</p>


# What is mcp-server-neon?

[![Rating](https://img.shields.io/badge/B-3775A9?label=Rating)](https://docs.anthropic.com/en/docs/build-with-claude/tool-use/implement-tool-use#best-practices-for-tool-definitions)
[![Helm](https://img.shields.io/badge/1.0.0-3775A9?logo=helm&label=Charts&logoColor=fff)](https://hub.docker.com/r/acuvity/mcp-server-neon/tags/)
[![Docker](https://img.shields.io/docker/image-size/acuvity/mcp-server-neon/0.4.0?logo=docker&logoColor=fff&label=0.4.0)](https://hub.docker.com/r/acuvity/mcp-server-neon)
[![PyPI](https://img.shields.io/badge/0.4.0-3775A9?logo=pypi&logoColor=fff&label=@neondatabase/mcp-server-neon)](https://github.com/neondatabase-labs/mcp-server-neon)
[![Scout](https://img.shields.io/badge/Active-3775A9?logo=docker&logoColor=fff&label=Scout)](https://hub.docker.com/r/acuvity/mcp-server-fetch/)
[![Install in VS Code Docker](https://img.shields.io/badge/VS_Code-One_click_install-0078d7?logo=githubcopilot)](https://insiders.vscode.dev/redirect/mcp/install?name=mcp-server-neon&config=%7B%22args%22%3A%5B%22run%22%2C%22-i%22%2C%22--rm%22%2C%22--read-only%22%2C%22-e%22%2C%22NEON_API_KEY%22%2C%22docker.io%2Facuvity%2Fmcp-server-neon%3A0.4.0%22%5D%2C%22command%22%3A%22docker%22%7D)

**Description:** Interact with Neon databases using natural language.

Packaged by Acuvity and published to our curated MCP server [registry](https://mcp.acuvity.ai) from @neondatabase/mcp-server-neon original [sources](https://github.com/neondatabase-labs/mcp-server-neon).

**Quick links:**

- [Integrate with your IDE](https://github.com/acuvity/mcp-servers-registry/blob/main/mcp-server-neon/docker/README.md#-clients-integrations)
- [Install with Docker](https://github.com/acuvity/mcp-servers-registry/tree/main/mcp-server-neon/docker/README.md#-run-it-with-docker)
- [Install with Helm](https://github.com/acuvity/mcp-servers-registry/tree/main/mcp-server-neon/charts/mcp-server-neon/README.md#how-to-install)

# Why We Built This

At [Acuvity](https://acuvity.ai), security is central to our mission‚Äîespecially for critical systems like MCP servers and integration in agentic systems.
To address this need, we've created a secure and robust Docker image designed to ensure @neondatabase/mcp-server-neon run reliably and safely.

## üîê Key Security Features

<details>
<summary>üì¶ Isolated Immutable Sandbox </summary>

- **Isolated Execution**: All tools run within secure, containerized sandboxes to enforce process isolation and prevent lateral movement.
- **Non-root by Default**: Enforces least-privilege principles, minimizing the impact of potential security breaches.
- **Read-only Filesystem**: Ensures runtime immutability, preventing unauthorized modification.
- **Version Pinning**: Guarantees consistency and reproducibility across deployments by locking tool and dependency versions.
- **CVE Scanning**: Continuously scans images for known vulnerabilities using [Docker Scout](https://docs.docker.com/scout/) to support proactive mitigation.
- **SBOM & Provenance**: Delivers full supply chain transparency by embedding metadata and traceable build information."
</details>

<details>
<summary>üõ°Ô∏è Runtime Security and Guardrails</summary>

**Minibridge Integration**: [Minibridge](https://github.com/acuvity/minibridge) establishes secure Agent-to-MCP connectivity, supports Rego/HTTP-based policy enforcement üïµÔ∏è, and simplifies orchestration.

The [ARC](https://github.com/acuvity/mcp-servers-registry/tree/main) container includes a [built-in Rego policy](https://github.com/acuvity/mcp-servers-registry/tree/main/mcp-server-neon/docker/policy.rego) that enables a set of runtime "guardrails"" to help enforce security, privacy, and correct usage of your services. Below is an overview of each guardrail provided.

### üîí Resource Integrity

**Mitigates MCP Rug Pull Attacks**

* **Goal:** Protect users from malicious tool description changes after initial approval, preventing post-installation manipulation or deception.
* **Mechanism:** Locks tool descriptions upon client approval and verifies their integrity before execution. Any modification to the description triggers a security violation, blocking unauthorized changes from server-side updates.

### üõ°Ô∏è Gardrails

### Covert Instruction Detection

Monitors incoming requests for hidden or obfuscated directives that could alter policy behavior.

* **Goal:** Stop attackers from slipping unnoticed commands or payloads into otherwise harmless data.
* **Mechanism:** Applies a library of regex patterns and binary‚Äêencoding checks to the full request body. If any pattern matches a known covert channel (e.g., steganographic markers, hidden HTML tags, escape-sequence tricks), the request is rejected.

### Sensitive Pattern Detection

Block user-defined sensitive data patterns (credential paths, filesystem references).

* **Goal:** Block accidental or malicious inclusion of sensitive information that violates data-handling rules.
* **Mechanism:** Runs a curated set of regexes against all payloads and tool descriptions‚Äîmatching patterns such as `.env` files, RSA key paths, directory traversal sequences.

### Shadowing Pattern Detection

Detects and blocks "shadowing" attacks, where a malicious MCP server sneaks hidden directives into its own tool descriptions to hijack or override the behavior of other, trusted tools.

* **Goal:** Stop a rogue server from poisoning the agent‚Äôs logic by embedding instructions that alter how a different server‚Äôs tools operate (e.g., forcing all emails to go to an attacker‚Äôs address even when the user calls a separate `send_email` tool).
* **Mechanism:** During policy load, each tool description is scanned for cross‚Äêtool override patterns‚Äîsuch as `<IMPORTANT>` sections referencing other tool names, hidden side‚Äêeffects, or directives that apply to a different server‚Äôs API. Any description that attempts to shadow or extend instructions for a tool outside its own namespace triggers a policy violation and is rejected.

### Schema Misuse Prevention

Enforces strict adherence to MCP input schemas.

* **Goal:** Prevent malformed or unexpected fields from bypassing validations, causing runtime errors, or enabling injections.
* **Mechanism:** Compares each incoming JSON object against the declared schema (required properties, allowed keys, types). Any extra, missing, or mistyped field triggers an immediate policy violation.

### Cross-Origin Tool Access

Controls whether tools may invoke tools or services from external origins.

* **Goal:** Prevent untrusted or out-of-scope services from being called.
* **Mechanism:** Examines tool invocation requests and outgoing calls, verifying each target against an allowlist of approved domains or service names. Calls to any non-approved origin are blocked.

### Secrets Redaction

Automatically masks sensitive values so they never appear in logs or responses.

* **Goal:** Ensure that API keys, tokens, passwords, and other credentials cannot leak in plaintext.
* **Mechanism:** Scans every text output for known secret formats (e.g., AWS keys, GitHub PATs, JWTs). Matches are replaced with `[REDACTED]` before the response is sent or recorded.

## Basic Authentication via Shared Secret

Provides a lightweight auth layer using a single shared token.

* **Mechanism:** Expects clients to send an `Authorization` header with the predefined secret.
* **Use Case:** Quickly lock down your endpoint in development or simple internal deployments‚Äîno complex OAuth/OIDC setup required.

These controls ensure robust runtime integrity, prevent unauthorized behavior, and provide a foundation for secure-by-design system operations.


To review the full policy, see it [here](https://github.com/acuvity/mcp-servers-registry/tree/main/mcp-server-neon/docker/policy.rego). Alternatively, you can override the default policy or supply your own policy file to use (see [here](https://github.com/acuvity/mcp-servers-registry/tree/main/mcp-server-neon/docker/entrypoint.sh) for Docker, [here](https://github.com/acuvity/mcp-servers-registry/tree/main/mcp-server-neon/charts/mcp-server-neon#minibridge) for Helm charts).

</details>

> [!NOTE]
> By default, all guardrails are turned off. You can enable or disable each one individually, ensuring that only the protections your environment needs are active.


# üì¶ How to Install


> [!TIP]
> Given mcp-server-neon scope of operation it can be hosted anywhere.

# üß∞ Clients Integrations

Below are the steps for configuring most clients that use MCP to elevate their Copilot experience.

> [!NOTE]
> These integrations function natively across all Minibridge modes.
> To keep things brief, only the docker local-run setup is covered here.

<details>
<summary>Visual Studio Code</summary>

To get started immediately, you can use the "one-click" link below:

[![Install in VS Code Docker](https://img.shields.io/badge/VS_Code-One_click_install-0078d7?logo=githubcopilot)](https://insiders.vscode.dev/redirect/mcp/install?name=mcp-server-neon&config=%7B%22args%22%3A%5B%22run%22%2C%22-i%22%2C%22--rm%22%2C%22--read-only%22%2C%22-e%22%2C%22NEON_API_KEY%22%2C%22docker.io%2Facuvity%2Fmcp-server-neon%3A0.4.0%22%5D%2C%22command%22%3A%22docker%22%7D)

## Global scope

Press `ctrl + shift + p` and type `Preferences: Open User Settings JSON` to add the following section:

```json
{
  "mcp": {
    "servers": {
      "acuvity-mcp-server-neon": {
        "env": {
          "NEON_API_KEY": "TO_BE_SET"
        },
        "command": "docker",
        "args": [
          "run",
          "-i",
          "--rm",
          "--read-only",
          "-e",
          "NEON_API_KEY",
          "docker.io/acuvity/mcp-server-neon:0.4.0"
        ]
      }
    }
  }
}
```

## Workspace scope

In your workspace create a file called `.vscode/mcp.json` and add the following section:

```json
{
  "servers": {
    "acuvity-mcp-server-neon": {
      "env": {
        "NEON_API_KEY": "TO_BE_SET"
      },
      "command": "docker",
      "args": [
        "run",
        "-i",
        "--rm",
        "--read-only",
        "-e",
        "NEON_API_KEY",
        "docker.io/acuvity/mcp-server-neon:0.4.0"
      ]
    }
  }
}
```

> To pass secrets you should use the `promptString` input type described in the [Visual Studio Code documentation](https://code.visualstudio.com/docs/copilot/chat/mcp-servers).

</details>

<details>
<summary>Windsurf IDE</summary>

In `~/.codeium/windsurf/mcp_config.json` add the following section:

```json
{
  "mcpServers": {
    "acuvity-mcp-server-neon": {
      "env": {
        "NEON_API_KEY": "TO_BE_SET"
      },
      "command": "docker",
      "args": [
        "run",
        "-i",
        "--rm",
        "--read-only",
        "-e",
        "NEON_API_KEY",
        "docker.io/acuvity/mcp-server-neon:0.4.0"
      ]
    }
  }
}
```

See [Windsurf documentation](https://docs.windsurf.com/windsurf/mcp) for more info.

</details>

<details>
<summary>Cursor IDE</summary>

Add the following JSON block to your mcp configuration file:
- `~/.cursor/mcp.json` for global scope
- `.cursor/mcp.json` for project scope

```json
{
  "mcpServers": {
    "acuvity-mcp-server-neon": {
      "env": {
        "NEON_API_KEY": "TO_BE_SET"
      },
      "command": "docker",
      "args": [
        "run",
        "-i",
        "--rm",
        "--read-only",
        "-e",
        "NEON_API_KEY",
        "docker.io/acuvity/mcp-server-neon:0.4.0"
      ]
    }
  }
}
```

See [cursor documentation](https://docs.cursor.com/context/model-context-protocol) for more information.

</details>
<details>

<summary>Claude Desktop</summary>

In the `claude_desktop_config.json` configuration file add the following section:

```json
{
  "mcpServers": {
    "acuvity-mcp-server-neon": {
      "env": {
        "NEON_API_KEY": "TO_BE_SET"
      },
      "command": "docker",
      "args": [
        "run",
        "-i",
        "--rm",
        "--read-only",
        "-e",
        "NEON_API_KEY",
        "docker.io/acuvity/mcp-server-neon:0.4.0"
      ]
    }
  }
}
```

See [Anthropic documentation](https://docs.anthropic.com/en/docs/agents-and-tools/mcp) for more information.
</details>

<details>
<summary>OpenAI python SDK</summary>

## Running locally

```python
async with MCPServerStdio(
    params={
        "env": {"NEON_API_KEY":"TO_BE_SET"},
        "command": "docker",
        "args": ["run","-i","--rm","--read-only","-e","NEON_API_KEY","docker.io/acuvity/mcp-server-neon:0.4.0"]
    }
) as server:
    tools = await server.list_tools()
```

## Running remotely

```python
async with MCPServerSse(
    params={
        "url": "http://<ip>:<port>/sse",
    }
) as server:
    tools = await server.list_tools()
```

See [OpenAI Agents SDK docs](https://openai.github.io/openai-agents-python/mcp/) for more info.

</details>

## üê≥ Run it with Docker

**Environment variables and secrets:**
  - `NEON_API_KEY` required to be set


<details>
<summary>Locally with STDIO</summary>

In your client configuration set:

- command: `docker`
- arguments: `run -i --rm --read-only -e NEON_API_KEY docker.io/acuvity/mcp-server-neon:0.4.0`

</details>

<details>
<summary>Locally with HTTP/sse</summary>

Simply run as:

```console
docker run -it -p 8000:8000 --rm --read-only -e NEON_API_KEY docker.io/acuvity/mcp-server-neon:0.4.0
```

Then on your application/client, you can configure to use it like:

```json
{
  "mcpServers": {
    "acuvity-mcp-server-neon": {
      "url": "http://localhost:8000/sse"
    }
  }
}
```

You might have to use different ports for different tools.

</details>

<details>
<summary>Remotely with Websocket tunneling and MTLS </summary>

> This section assume you are familiar with TLS and certificates and will require:
> - a server certificate with proper DNS/IP field matching your tool deployment.
> - a client-ca used to sign client certificates

1. Start the server in `backend` mode
 - add an environment variable like `-e MINIBRIDGE_MODE=backend`
 - add the TLS certificates (recommended) through a volume let's say `/certs` ex (`-v $PWD/certs:/certs`)
 - instruct minibridge to use those certs with
   - `-e MINIBRIDGE_TLS_SERVER_CERT=/certs/server-cert.pem`
   - `-e MINIBRIDGE_TLS_SERVER_KEY=/certs/server-key.pem`
   - `-e MINIBRIDGE_TLS_SERVER_KEY_PASS=optional`
   - `-e MINIBRIDGE_TLS_SERVER_CLIENT_CA=/certs/client-ca.pem`

2. Start `minibridge` locally in frontend mode:
  - Get [minibridge](https://github.com/acuvity/minibridge) binary for your OS.

In your client configuration, Minibridge works like any other STDIO command.

Example for Claude Desktop:

```json
{
  "mcpServers": {
    "acuvity-mcp-server-neon": {
      "command": "minibridge",
      "args": ["frontend", "--backend", "wss://<remote-url>:8000/ws", "--tls-client-backend-ca", "/path/to/ca/that/signed/the/server-cert.pem/ca.pem", "--tls-client-cert", "/path/to/client-cert.pem", "--tls-client-key", "/path/to/client-key.pem"]
    }
  }
}
```

That's it.

Minibridge offers a host of additional features. For step-by-step guidance, please visit the wiki. And if anything‚Äôs unclear, don‚Äôt hesitate to reach out!

</details>

## üõ°Ô∏è Runtime security

**Guardrails:**

To activate guardrails in your Docker containers, define the `GUARDRAILS` environment variable with the protections you need. Available options:
- covert-instruction-detection
- sensitive-pattern-detection
- shadowing-pattern-detection
- schema-misuse-prevention
- cross-origin-tool-access
- secrets-redaction

For example adding:
- `-e GUARDRAILS="secrets-redaction covert-instruction-detection"`
to your docker arguments will enable the `secrets-redaction` and `covert-instruction-detection` guardrails.

**Basic Authentication:**

To turn on Basic Authentication, add `BASIC_AUTH_SECRET` like:
- `-e BASIC_AUTH_SECRET="supersecret"`
to your docker arguments. This will enable the Basic Authentication check.

Then you can connect through `http/sse` as usual given that you pass an `Authorization: Bearer supersecret` header with your secret as Bearer token.

> [!CAUTION]
> While basic auth will protect against unauthorized access, you should use it only in controlled environment,
> rotate credentials frequently and **always** use TLS.

## ‚òÅÔ∏è Deploy On Kubernetes

<details>
<summary>Deploy using Helm Charts</summary>

### Chart settings requirements

This chart requires some mandatory information to be installed.

**Mandatory Secrets**:
  - `NEON_API_KEY` secret to be set as secrets.NEON_API_KEY either by `.value` or from existing with `.valueFrom`

### How to install

You can inspect the chart `README`:

```console
helm show readme oci://docker.io/acuvity/mcp-server-neon --version 1.0.0
````

You can inspect the values that you can configure:

```console
helm show values oci://docker.io/acuvity/mcp-server-neon --version 1.0.0
````

Install with helm

```console
helm install mcp-server-neon oci://docker.io/acuvity/mcp-server-neon --version 1.0.0
```

From there your MCP server mcp-server-neon will be reachable by default through `http/sse` from inside the cluster using the Kubernetes Service `mcp-server-neon` on port `8000` by default. You can change that by looking at the `service` section of the `values.yaml` file.

### How to Monitor

The deployment will create a Kubernetes service with a `healthPort`, that is used for liveness probes and readiness probes. This health port can also be used by the monitoring stack of your choice and exposes metrics under the `/metrics` path.

See full charts [Readme](https://github.com/acuvity/mcp-servers-registry/tree/main/mcp-server-neon/charts/mcp-server-neon/README.md) for more details about settings and runtime security including guardrails activation.

</details>

# üß† Server features

## üß∞ Tools (20)
<details>
<summary>list_projects</summary>

**Description**:

```
Lists the first 10 Neon projects in your account. If you can't find the project, increase the limit by passing a higher value to the `limit` parameter.
```

**Parameter**:

| Name | Type | Description | Required? |
|-----------|------|-------------|-----------|
| params | object | not set | No
</details>
<details>
<summary>create_project</summary>

**Description**:

```
Create a new Neon project. If someone is trying to create a database, use this tool.
```

**Parameter**:

| Name | Type | Description | Required? |
|-----------|------|-------------|-----------|
| params | object | not set | No
</details>
<details>
<summary>delete_project</summary>

**Description**:

```
Delete a Neon project
```

**Parameter**:

| Name | Type | Description | Required? |
|-----------|------|-------------|-----------|
| params | object | not set | No
</details>
<details>
<summary>describe_project</summary>

**Description**:

```
Describes a Neon project
```

**Parameter**:

| Name | Type | Description | Required? |
|-----------|------|-------------|-----------|
| params | object | not set | No
</details>
<details>
<summary>run_sql</summary>

**Description**:

```

    <use_case>
      Use this tool to execute a single SQL statement against a Neon database.
    </use_case>

    <important_notes>
      If you have a temporary branch from a prior step, you MUST:
      1. Pass the branch ID to this tool unless explicitly told otherwise
      2. Tell the user that you are using the temporary branch with ID [branch_id]
    </important_notes>
                 
```

**Parameter**:

| Name | Type | Description | Required? |
|-----------|------|-------------|-----------|
| params | object | not set | No
</details>
<details>
<summary>run_sql_transaction</summary>

**Description**:

```

    <use_case>
      Use this tool to execute a SQL transaction against a Neon database, should be used for multiple SQL statements.
    </use_case>

    <important_notes>
      If you have a temporary branch from a prior step, you MUST:
      1. Pass the branch ID to this tool unless explicitly told otherwise
      2. Tell the user that you are using the temporary branch with ID [branch_id]
    </important_notes>
                 
```

**Parameter**:

| Name | Type | Description | Required? |
|-----------|------|-------------|-----------|
| params | object | not set | No
</details>
<details>
<summary>describe_table_schema</summary>

**Description**:

```
Describe the schema of a table in a Neon database
```

**Parameter**:

| Name | Type | Description | Required? |
|-----------|------|-------------|-----------|
| params | object | not set | No
</details>
<details>
<summary>get_database_tables</summary>

**Description**:

```
Get all tables in a Neon database
```

**Parameter**:

| Name | Type | Description | Required? |
|-----------|------|-------------|-----------|
| params | object | not set | No
</details>
<details>
<summary>create_branch</summary>

**Description**:

```
Create a branch in a Neon project
```

**Parameter**:

| Name | Type | Description | Required? |
|-----------|------|-------------|-----------|
| params | object | not set | No
</details>
<details>
<summary>prepare_database_migration</summary>

**Description**:

```

  <use_case>
    This tool performs database schema migrations by automatically generating and executing DDL statements.
    
    Supported operations:
    CREATE operations:
    - Add new columns (e.g., "Add email column to users table")
    - Create new tables (e.g., "Create posts table with title and content columns")
    - Add constraints (e.g., "Add unique constraint on users.email")

    ALTER operations:
    - Modify column types (e.g., "Change posts.views to bigint")
    - Rename columns (e.g., "Rename user_name to username in users table")
    - Add/modify indexes (e.g., "Add index on posts.title")
    - Add/modify foreign keys (e.g., "Add foreign key from posts.user_id to users.id")

    DROP operations:
    - Remove columns (e.g., "Drop temporary_field from users table")
    - Drop tables (e.g., "Drop the old_logs table")
    - Remove constraints (e.g., "Remove unique constraint from posts.slug")

    The tool will:
    1. Parse your natural language request
    2. Generate appropriate SQL
    3. Execute in a temporary branch for safety
    4. Verify the changes before applying to main branch

    Project ID and database name will be automatically extracted from your request.
    If the database name is not provided, the default neondb or first available database is used.
  </use_case>

  <workflow>
    1. Creates a temporary branch
    2. Applies the migration SQL in that branch
    3. Returns migration details for verification
  </workflow>

  <important_notes>
    After executing this tool, you MUST:
    1. Test the migration in the temporary branch using the 'run_sql' tool
    2. Ask for confirmation before proceeding
    3. Use 'complete_database_migration' tool to apply changes to main branch
  </important_notes>

  <example>
    For a migration like:
    ALTER TABLE users ADD COLUMN last_login TIMESTAMP;
    
    You should test it with:
    SELECT column_name, data_type 
    FROM information_schema.columns 
    WHERE table_name = 'users' AND column_name = 'last_login';
    
    You can use 'run_sql' to test the migration in the temporary branch that this
    tool creates.
  </example>


  <next_steps>
  After executing this tool, you MUST follow these steps:
    1. Use 'run_sql' to verify changes on temporary branch
    2. Follow these instructions to respond to the client: 

      <response_instructions>
        <instructions>
          Provide a brief confirmation of the requested change and ask for migration commit approval.

          You MUST include ALL of the following fields in your response:
          - Migration ID (this is required for commit and must be shown first)  
          - Temporary Branch Name (always include exact branch name)
          - Temporary Branch ID (always include exact ID)
          - Migration Result (include brief success/failure status)

          Even if some fields are missing from the tool's response, use placeholders like "not provided" rather than omitting fields.
        </instructions>

        <do_not_include>
          IMPORTANT: Your response MUST NOT contain ANY technical implementation details such as:
          - Data types (e.g., DO NOT mention if a column is boolean, varchar, timestamp, etc.)
          - Column specifications or properties
          - SQL syntax or statements
          - Constraint definitions or rules
          - Default values
          - Index types
          - Foreign key specifications
          
          Keep the response focused ONLY on confirming the high-level change and requesting approval.
          
          <example>
            INCORRECT: "I've added a boolean is_published column to the posts table..."
            CORRECT: "I've added the is_published column to the posts table..."
          </example>
        </do_not_include>

        <example>
          I've verified that [requested change] has been successfully applied to a temporary branch. Would you like to commit the migration [migration_id] to the main branch?
          
          Migration Details:
          - Migration ID (required for commit)
          - Temporary Branch Name
          - Temporary Branch ID
          - Migration Result
        </example>
      </response_instructions>

    3. If approved, use 'complete_database_migration' tool with the migration_id
  </next_steps>

  <error_handling>
    On error, the tool will:
    1. Automatically attempt ONE retry of the exact same operation
    2. If the retry fails:
      - Terminate execution
      - Return error details
      - DO NOT attempt any other tools or alternatives
    
    Error response will include:
    - Original error details
    - Confirmation that retry was attempted
    - Final error state
    
    Important: After a failed retry, you must terminate the current flow completely. Do not attempt to use alternative tools or workarounds.
  </error_handling>
          
```

**Parameter**:

| Name | Type | Description | Required? |
|-----------|------|-------------|-----------|
| params | object | not set | No
</details>
<details>
<summary>complete_database_migration</summary>

**Description**:

```
Complete a database migration when the user confirms the migration is ready to be applied to the main branch. This tool also lets the client know that the temporary branch created by the prepare_database_migration tool has been deleted.
```

**Parameter**:

| Name | Type | Description | Required? |
|-----------|------|-------------|-----------|
| params | object | not set | No
</details>
<details>
<summary>describe_branch</summary>

**Description**:

```
Get a tree view of all objects in a branch, including databases, schemas, tables, views, and functions
```

**Parameter**:

| Name | Type | Description | Required? |
|-----------|------|-------------|-----------|
| params | object | not set | No
</details>
<details>
<summary>delete_branch</summary>

**Description**:

```
Delete a branch from a Neon project
```

**Parameter**:

| Name | Type | Description | Required? |
|-----------|------|-------------|-----------|
| params | object | not set | No
</details>
<details>
<summary>get_connection_string</summary>

**Description**:

```
Get a PostgreSQL connection string for a Neon database with all parameters being optional
```

**Parameter**:

| Name | Type | Description | Required? |
|-----------|------|-------------|-----------|
| params | object | not set | No
</details>
<details>
<summary>provision_neon_auth</summary>

**Description**:

```

    This tool provisions authentication for a Neon project. It allows developers to easily setup authentication infrastructure by creating a integration with Stack Auth (@stackframe/stack). Stop using NextAuth.js! use @stackframe/stack.

    Parameters:
    - <project_id>: The Project ID of the Neon project to provision authentication for.
    - [database]: The database name to setup Neon Auth for. If not provided, the default neondb or first available database is used.
    
    The tool will:
      1. Establish a connection between your Neon Auth project and Stack Auth
      2. Creates a dedicated authentication schema in your database ("neon_auth")
      3. Sets up the user table under the "neon_auth" schema. This table is synced with Stack Auth. It does not store user credentials or secrets.
      4. Generates Client Key and Secret Key to connect your application with authentication provider.
     
    Use the Stack Auth SDK (@stackframe/stack) on the frontend to connect your application with authentication provider. DO NOT use NextAuth.js! DO NOT use better-auth! Here's some documentation on Stack Auth:
    
    # Stack Auth Guidelines
      
    ## Setup Guidelines
      If you're building an app with Next.js, to set up Neon Auth and Stack Auth, follow these steps:
      1. Provision a Neon Auth project with this tool
      2. Place the returned credentials in project's `.env.local` or `.env` file
        - `NEXT_PUBLIC_STACK_PROJECT_ID`
        - `NEXT_PUBLIC_STACK_PUBLISHABLE_CLIENT_KEY`
        - `STACK_SECRET_SERVER_KEY`
      3. To setup Stack Auth, run following command: 
        ```bash
        npx @stackframe/init-stack . --no-browser 
        ```
        This command will automaticallysetup the project with - 
        - It will add `@stackframe/stack` dependency to `package.json`
        - It will create a `stack.ts` file in your project to setup `StackServerApp`. 
        - It will wrap the root layout with `StackProvider` and `StackTheme`
        - It will create root Suspense boundary `app/loading.tsx` to handle loading state while Stack is fetching user data.
        - It will also create `app/handler/[...stack]/page.tsx` file to handle auth routes like sign in, sign up, forgot password, etc.
      4. Do not try to manually create any of these files or directories. Do not try to create SignIn, SignUp, or UserButton components manually, instead use the ones provided by `@stackframe/stack`.
      
      
    ## Components Guidelines
      - Use pre-built components from `@stackframe/stack` like `<UserButton />`, `<SignIn />`, and `<SignUp />` to quickly set up auth UI.
      - You can also compose smaller pieces like `<OAuthButtonGroup />`, `<MagicLinkSignIn />`, and `<CredentialSignIn />` for custom flows.
      - Example:
        
        ```tsx
        import { SignIn } from '@stackframe/stack';
        export default function Page() {
          return <SignIn />;
        }
        ```

    ## User Management Guidelines
      - In Client Components, use the `useUser()` hook to retrieve the current user (it returns `null` when not signed in).
      - Update user details using `user.update({...})` and sign out via `user.signOut()`.
      - For pages that require a user, call `useUser({ or: "redirect" })` so unauthorized visitors are automatically redirected.
    
    ## Client Component Guidelines
      - Client Components rely on hooks like `useUser()` and `useStackApp()`.
      - Example:
        
        ```tsx
        "use client";
        import { useUser } from "@stackframe/stack";
        export function MyComponent() {
          const user = useUser();
          return <div>{user ? `Hello, ${user.displayName}` : "Not logged in"}</div>;
        }
        ```
      
    ## Server Component Guidelines
      - For Server Components, use `stackServerApp.getUser()` from your `stack.ts` file.
      - Example:
        
        ```tsx
        import { stackServerApp } from "@/stack";
        export default async function ServerComponent() {
          const user = await stackServerApp.getUser();
          return <div>{user ? `Hello, ${user.displayName}` : "Not logged in"}</div>;
        }
        ```
    
    ## Page Protection Guidelines
      - Protect pages by:
        - Using `useUser({ or: "redirect" })` in Client Components.
        - Using `await stackServerApp.getUser({ or: "redirect" })` in Server Components.
        - Implementing middleware that checks for a user and redirects to `/handler/sign-in` if not found.
      - Example middleware:
        
        ```tsx
        export async function middleware(request: NextRequest) {
          const user = await stackServerApp.getUser();
          if (!user) {
            return NextResponse.redirect(new URL('/handler/sign-in', request.url));
          }
          return NextResponse.next();
        }
        export const config = { matcher: '/protected/:path*' };
        ```
      
      ```
      ## Examples
      ### Example: custom-profile-page
      #### Task
      Create a custom profile page that:
      - Displays the user's avatar, display name, and email.
      - Provides options to sign out.
      - Uses Stack Auth components and hooks.
      #### Response
      ##### File: app/profile/page.tsx
      ###### Code
      ```tsx
      'use client';
      import { useUser, useStackApp, UserButton } from '@stackframe/stack';
      export default function ProfilePage() {
        const user = useUser({ or: "redirect" });
        const app = useStackApp();
        return (
          <div>
            <UserButton />
            <h1>Welcome, {user.displayName || "User"}</h1>
            <p>Email: {user.primaryEmail}</p>
            <button onClick={() => user.signOut()}>Sign Out</button>
          </div>
        );
      }
      ```
        
```

**Parameter**:

| Name | Type | Description | Required? |
|-----------|------|-------------|-----------|
| params | object | not set | No
</details>
<details>
<summary>explain_sql_statement</summary>

**Description**:

```
Describe the PostgreSQL query execution plan for a query of SQL statement by running EXPLAIN (ANAYLZE...) in the database
```

**Parameter**:

| Name | Type | Description | Required? |
|-----------|------|-------------|-----------|
| params | object | not set | No
</details>
<details>
<summary>prepare_query_tuning</summary>

**Description**:

```

  <use_case>
    This tool helps developers improve PostgreSQL query performance for slow queries or DML statements by analyzing execution plans and suggesting optimizations.
    
    The tool will:
    1. Create a temporary branch for testing optimizations and remember the branch ID
    2. Extract and analyze the current query execution plan
    3. Extract all fully qualified table names (schema.table) referenced in the plan 
    4. Gather detailed schema information for each referenced table using describe_table_schema
    5. Suggest and implement improvements like:
      - Adding or modifying indexes based on table schemas and query patterns
      - Query structure modifications
      - Identifying potential performance bottlenecks
    6. Apply the changes to the temporary branch using run_sql
    7. Compare performance before and after changes (but ONLY on the temporary branch passing branch ID to all tools)
    8. Continue with next steps using complete_query_tuning tool (on main branch)
    
    Project ID and database name will be automatically extracted from your request.
    The temporary branch ID will be added when invoking other tools.
    Default database is neondb if not specified.

    IMPORTANT: This tool is part of the query tuning workflow. Any suggested changes (like creating indexes) must first be applied to the temporary branch using the 'run_sql' tool.
    and then to the main branch using the 'complete_query_tuning' tool, NOT the 'prepare_database_migration' tool. 
    To apply using the 'complete_query_tuning' tool, you must pass the tuning_id, NOT the temporary branch ID to it.
  </use_case>

  <workflow>
    1. Creates a temporary branch
    2. Analyzes current query performance and extracts table information
    3. Implements and tests improvements (using tool run_sql for schema modifications and explain_sql_statement for performance analysis, but ONLY on the temporary branch created in step 1 passing the same branch ID to all tools)
    4. Returns tuning details for verification
  </workflow>

  <important_notes>
    After executing this tool, you MUST:
    1. Review the suggested changes
    2. Verify the performance improvements on temporary branch - by applying the changes with run_sql and running explain_sql_statement again)
    3. Decide whether to keep or discard the changes
    4. Use 'complete_query_tuning' tool to apply or discard changes to the main branch
    
    DO NOT use 'prepare_database_migration' tool for applying query tuning changes.
    Always use 'complete_query_tuning' to ensure changes are properly tracked and applied.

    Note: 
    - Some operations like creating indexes can take significant time on large tables
    - Table statistics updates (ANALYZE) are NOT automatically performed as they can be long-running
    - Table statistics maintenance should be handled by PostgreSQL auto-analyze or scheduled maintenance jobs
    - If statistics are suspected to be stale, suggest running ANALYZE as a separate maintenance task
  </important_notes>

  <example>
    For a query like:
    SELECT o.*, c.name 
    FROM orders o 
    JOIN customers c ON c.id = o.customer_id 
    WHERE o.status = 'pending' 
    AND o.created_at > '2024-01-01';
    
    The tool will:
    1. Extract referenced tables: public.orders, public.customers
    2. Gather schema information for both tables
    3. Analyze the execution plan
    4. Suggest improvements like:
       - Creating a composite index on orders(status, created_at)
       - Optimizing the join conditions
    5. If confirmed, apply the suggested changes to the temporary branch using run_sql
    6. Compare execution plans and performance before and after changes (but ONLY on the temporary branch passing branch ID to all tools)
    
  </example>

  <next_steps>
  After executing this tool, you MUST follow these steps:
    1. Review the execution plans and suggested changes
    2. Follow these instructions to respond to the client: 

      <response_instructions>
        <instructions>
          Provide a brief summary of the performance analysis and ask for approval to apply changes on the temporary branch.

          You MUST include ALL of the following fields in your response:
          - Tuning ID (this is required for completion)
          - Temporary Branch Name
          - Temporary Branch ID
          - Original Query Cost
          - Improved Query Cost
          - Referenced Tables (list all tables found in the plan)
          - Suggested Changes

          Even if some fields are missing from the tool's response, use placeholders like "not provided" rather than omitting fields.
        </instructions>

        <do_not_include>
          IMPORTANT: Your response MUST NOT contain ANY technical implementation details such as:
          - Exact index definitions
          - Internal PostgreSQL settings
          - Complex query rewrites
          - Table partitioning details
          
          Keep the response focused on high-level changes and performance metrics.
        </do_not_include>

        <example>
          I've analyzed your query and found potential improvements that could reduce execution time by [X]%.
          Would you like to apply these changes to improve performance?
          
          Analysis Details:
          - Tuning ID: [id]
          - Temporary Branch: [name]
          - Branch ID: [id]
          - Original Cost: [cost]
          - Improved Cost: [cost]
          - Referenced Tables:
            * public.orders
            * public.customers
          - Suggested Changes:
            * Add index for frequently filtered columns
            * Optimize join conditions

          To apply these changes, I will use the 'complete_query_tuning' tool after your approval and pass the tuning_id, NOT the temporary branch ID to it.
        </example>
      </response_instructions>

    3. If approved, use ONLY the 'complete_query_tuning' tool with the tuning_id
  </next_steps>

  <error_handling>
    On error, the tool will:
    1. Automatically attempt ONE retry of the exact same operation
    2. If the retry fails:
      - Terminate execution
      - Return error details
      - Clean up temporary branch
      - DO NOT attempt any other tools or alternatives
    
    Error response will include:
    - Original error details
    - Confirmation that retry was attempted
    - Final error state
    
    Important: After a failed retry, you must terminate the current flow completely.
  </error_handling>
    
```

**Parameter**:

| Name | Type | Description | Required? |
|-----------|------|-------------|-----------|
| params | object | not set | No
</details>
<details>
<summary>complete_query_tuning</summary>

**Description**:

```
Complete a query tuning session by either applying the changes to the main branch or discarding them. 
    <important_notes>
        BEFORE RUNNING THIS TOOL: test out the changes in the temporary branch first by running 
        - 'run_sql' with the suggested DDL statements.
        - 'explain_sql_statement' with the original query and the temporary branch.
        This tool is the ONLY way to finally apply changes afterthe 'prepare_query_tuning' tool to the main branch.
        You MUST NOT use 'prepare_database_migration' or other tools to apply query tuning changes.
        You MUST pass the tuning_id obtained from the 'prepare_query_tuning' tool, NOT the temporary branch ID as tuning_id to this tool.
        You MUSt pass the temporary branch ID used in the 'prepare_query_tuning' tool as TEMPORARY branchId to this tool.
        The tool OPTIONALLY receives a second branch ID or name which can be used instead of the main branch to apply the changes.
        This tool MUST be called after tool 'prepare_query_tuning' even when the user rejects the changes, to ensure proper cleanup of temporary branches.
    </important_notes>    

    This tool:
    1. Applies suggested changes (like creating indexes) to the main branch (or specified branch) if approved
    2. Handles cleanup of temporary branch
    3. Must be called even when changes are rejected to ensure proper cleanup

    Workflow:
    1. After 'prepare_query_tuning' suggests changes
    2. User reviews and approves/rejects changes
    3. This tool is called to either:
      - Apply approved changes to main branch and cleanup
      - OR just cleanup if changes are rejected
                 
```

**Parameter**:

| Name | Type | Description | Required? |
|-----------|------|-------------|-----------|
| params | object | not set | No
</details>
<details>
<summary>list_slow_queries</summary>

**Description**:

```

    <use_case>
      Use this tool to list slow queries from your Neon database.
    </use_case>

    <important_notes>
      This tool queries the pg_stat_statements extension to find queries that are taking longer than expected.
      The tool will return queries sorted by execution time, with the slowest queries first.
    </important_notes>
                 
```

**Parameter**:

| Name | Type | Description | Required? |
|-----------|------|-------------|-----------|
| params | object | not set | No
</details>
<details>
<summary>list_branch_computes</summary>

**Description**:

```
Lists compute endpoints for a project or specific branch
```

**Parameter**:

| Name | Type | Description | Required? |
|-----------|------|-------------|-----------|
| params | object | not set | No
</details>

## üìö Resources (3)

<details>
<summary>Resources</summary>

| Name | Mime type | URI| Content |
|-----------|------|-------------|-----------|
| neon-auth | text/plain | https://github.com/neondatabase-labs/ai-rules/blob/main/neon-auth.mdc | - |
| neon-serverless | text/plain | https://github.com/neondatabase-labs/ai-rules/blob/main/neon-serverless.mdc | - |
| neon-drizzle | text/plain | https://github.com/neondatabase-labs/ai-rules/blob/main/neon-drizzle.mdc | - |

</details>


# üîê Resource SBOM

Minibridge will perform hash checks for the following resources. The hashes are given as references and are the sha256 sum of the description.

| Resource | Name | Parameter | Hash |
|-----------|------|------|------|
| tools | complete_database_migration | description | b736f1ea9fbdc4fec5bc6c42c6153bd6ad03bf5383d2af8f1b53d87f51ed133f |
| tools | complete_query_tuning | description | 74582223f9dbf428bee2d64222f1d0c62a38540c6047442bf7ba2a0b94fbd2b6 |
| tools | create_branch | description | 4b60af1449116c87c8c3e76f48c07d2d2da516d9ef6af6dea6c1d19c48d6bfb1 |
| tools | create_project | description | 93e4181b7ceb6ec10d1308d293e20278e264d9c5441c6e71b008324dfa292a49 |
| tools | delete_branch | description | 634433096d256a32c172240a6b698349114ebf200a81ea03785e7064f9afea97 |
| tools | delete_project | description | c1c928840b71df4d03ec8049efce230ab2018b8e8e725787cbe4215eed5695a3 |
| tools | describe_branch | description | 1a8464355b25d521dbe63e25698df0a995d3b48678e2b6feb81a4a82c10cea53 |
| tools | describe_project | description | 639afcfdfcbb66c71dcde467532a0ccd0f9ab6cef7e9a5a350438e41a1e52150 |
| tools | describe_table_schema | description | 45bdb43f412b3ada5cd67fe485f021a3593439d60e605e8eecf1c13392ee473d |
| tools | explain_sql_statement | description | 1ee1dab295a65e2a86d2c7c5e978eb4382688938c5263072492d3163d4e2d4fa |
| tools | get_connection_string | description | 8828151c1ddd982f84746783bdd8f6377e548452788258b5b139e9aa7b26ee55 |
| tools | get_database_tables | description | b2fe1dd39ed093b79180f4cf2188ec1591843672448e0ccbf21b1e2b3ba9631f |
| tools | list_branch_computes | description | 39cb1a7a16b28c8f90328f90552da452bab52311445c1fd87b14a4509a14983a |
| tools | list_projects | description | 31984d294a197b32fb126399a6118f6e4a69d77ebe3d73cb519daf808d017783 |
| tools | list_slow_queries | description | 72d538a1cf53fbacba8b63fa48bef9bfac6e5bdaf1841ca9372add1dca368a36 |
| tools | prepare_database_migration | description | 0b614563d1078247b297e1a3bed29eedd673755feecacebcb7dba54e241b65cb |
| tools | prepare_query_tuning | description | 3e312d73e5b7a4b4f111d45d63ae6278cb9adb3a000e30b5a9c68ae3a9130ebb |
| tools | provision_neon_auth | description | d17b20d370cf497d03dbf3387599679a8eee7eb4b6e7dcc3a2e9c9973e0aa071 |
| tools | run_sql | description | 4be63b91c49994dd0966d3a78db4d4ba521e5b0505cd07c7653386d8b93521c4 |
| tools | run_sql_transaction | description | 467d3961e89cdc73909e795ba4899c51ee62b153a86c94ee7d8fd05f7a0cf2c8 |


üí¨ Questions? Open an issue or contact [ support@acuvity.ai ](mailto:support@acuvity.ai).
üì¶ Contributions welcome!
